
\documentclass[compress]{beamer}
\usefonttheme{professionalfonts}

\input{course_style}

\newcommand{\gfx}[2]{
\begin{center}
	\includegraphics[width=#2\linewidth]{memory/#1}
\end{center}
}
\title{Frameworks}
\date{LSTM Variants}

\begin{document}


\frame{\titlepage
}

\begin{frame}{What's most important part of LSTM}

  \only<1>{
 Greff et al. explore
    \begin{itemize}
\item No Input Gate (NIG)
\item No Forget Gate (NFG)
\item No Output Gate (NOG)
\item No Input Activation Function (NIAF)
\item No Output Activation Function (NOAF)
\item No Peepholes (NP)
\item Coupled Input and Forget Gate (CIFG) : GRU, $f_t = 1 - i_t$
\item Full Gate Recurrence (FGR): Original LSTM paper
    \end{itemize}

}
  \only<2>{\gfx{perf-vs-params}{.75}}

\end{frame}

\begin{frame}{GRU simplifies slightly}

  \only<1>{\gfx{LSTM-removal}{.8}

  }


  \only<2>{\gfx{LSTM3-var-GRU}{.9}

  Slightly fewer parameters}

\end{frame}

\begin{frame}{Bi-directional LSTMs}

  \gfx{RNN-bidirectional}{.8}

  Simple extension, often slightly improve performance (but don't
  always make sense for task)
\end{frame}


\begin{frame}{Comparing architechtures}

  \begin{itemize}
    \item GRUs seem competitive
    \item LSTM seems to be good tradeoff
    \item Bi-directional often offers slight improvement
  \end{itemize}

\end{frame}


\begin{frame}{Why not convolutional nets?}

  \begin{itemize}
    \item Doesn't make sense linguistically
    \item Pretty expensive
    \item LSTMs usually work about as well
  \pause
  \item GRU convnet provides better mechanism
  \end{itemize}

  \gfx{GRU-tree}{.5}

\end{frame}

\end{document}
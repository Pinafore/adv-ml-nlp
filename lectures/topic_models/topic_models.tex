

\providecommand{\graphscale}{0.6}




\begin{frame}

	\frametitle{Why topic models?}

	\begin{columns}

	\column{.3\linewidth}

	\includegraphics[width=1\linewidth]{topic_models/newspapers}

	\column{.55\linewidth}

	\begin{itemize}
		\item Suppose you have a huge number of documents
		\item Want to know what's going on
		\item Can't read them all (e.g. every New York Times article from the 90's)
		\item Topic models offer a way to get a corpus-level view of major themes
		\pause
		\item Unsupervised
	\end{itemize}


	\end{columns}

\end{frame}

\begin{frame}{Roadmap}

	\begin{itemize}
		\item What are topic models
		\item How to go from raw data to topics
	\end{itemize}

\end{frame}


\frame{
\begin{center}
\frametitle{Embedding Space}
From an \textbf<1>{input corpus} and number of topics \textbf<1>{$K$} $\rightarrow$ \textbf<2>{words to topics} \\
\only<1>{\includegraphics[width=0.6\linewidth]{topic_models/reading_tea_leaves/heldout_0} }
\only<2>{\includegraphics[width=0.9\linewidth]{topic_models/reading_tea_leaves/nyt_topics_wide}}
\end{center}
}

\frame{\frametitle{Conceptual Approach}

\begin{itemize}
\item For each document, what topics are expressed by that document?

\begin{center}
\includegraphics[width=0.9\linewidth]{topic_models/nyt_documents}
\end{center}

\end{itemize}
}



\begin{frame}

\frametitle{Topics from \emph{Science}}

\begin{center}
\includegraphics[width=0.8\linewidth]{topic_models/example_topics}
\end{center}

\end{frame}


\begin{frame}

\frametitle{Why should you care?}

\begin{itemize}
\item Neat way to explore / understand corpus collections
\begin{itemize}
	\item E-discovery
	\item Social media
	\item Scientific data
\end{itemize}
\item NLP Applications
\begin{itemize}
   \item Word Sense Disambiguation
   \item Discourse Segmentation
   \item Machine Translation
\end{itemize}
\item Psychology: word meaning, polysemy
\item Inference is (relatively) simple
\end{itemize}

\end{frame}

\frame
{
  \frametitle{Matrix Factorization Approach}

\begin{center}
\includegraphics[width=0.9\linewidth]{topic_models/factorization.pdf}
\end{center}

\begin{columns}
\column{.5\textwidth}
\begin{block}{}
	\begin{itemize}
		\item[K] Number of topics
		\item[M] Number of documents
		\item[V] Size of vocabulary
	\end{itemize}
\end{block}
\column{.5\textwidth}
\pause
\begin{itemize}
\item If you use singular value decomposition (SVD), this technique is called latent semantic analysis.
\item Popular in information retrieval.
\end{itemize}
\end{columns}

}

\begin{frame}

\frametitle{Alternative: Generative Model}

\begin{itemize}
  \item How your data came to be
  \item Sequence of Probabilistic Steps
  \item Posterior Inference
    \pause
  \item Blei, Ng, Jordan.  Latent {\bf Dirichlet} Allocation.  JMLR, 2003.
\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Multinomial Distribution}

	\begin{itemize}
		\item Distribution over discrete outcomes
		\item Represented by non-negative vector that sums to one
		\item Picture representation
	\begin{center}
\includegraphics[width=0.4\linewidth]{topic_models/multinomial}
	\end{center}
		\pause
		\item Come from a Dirichlet distribution

	\end{itemize}


\end{frame}

\begin{frame}

\frametitle{Dirichlet Distribution}

\begin{center}
\includegraphics[width=0.4\linewidth]{topic_models/equations/dirichlet} \\ \bigskip
\pause
\includegraphics[width=0.6\linewidth]{topic_models/dirichlet_1} \\
\includegraphics[width=0.2\linewidth]{topic_models/equations/dirichlet_params_1} \includegraphics[width=0.2\linewidth]{topic_models/equations/dirichlet_params_2} \includegraphics[width=0.2\linewidth]{topic_models/equations/dirichlet_params_3} \\
\pause
\includegraphics[width=0.6\linewidth]{topic_models/dirichlet_2} \\
\includegraphics[width=0.2\linewidth]{topic_models/equations/dirichlet_params_4} \includegraphics[width=0.2\linewidth]{topic_models/equations/dirichlet_params_5} \includegraphics[width=0.2\linewidth]{topic_models/equations/dirichlet_params_6} \\
\end{center}

\end{frame}

\begin{frame}
\frametitle{Dirichlet Distribution}
\begin{center}
\includegraphics[width=0.5\linewidth]{topic_models/sparsity}
\end{center}
\end{frame}



\begin{frame}
\frametitle{Dirichlet Distribution}
\begin{itemize}
  \item If ${\bm \phi} \sim \dir(\alpha)$, ${\bm w} \sim \mult(\phi)$, and $n_k = |\{ w_i : w_i = k\}|$ then
  \begin{align}
  	p(\phi | \alpha, {\bm w}) & \propto p({\bm w} | \phi) p(\phi | \alpha) \\
	                       & \propto  \prod_{k} \phi^{n_k} \pause  \prod_k { \phi^{\alpha_k - 1}} \\
	                       & \propto \prod_k \phi^{\alpha_k + n_k - 1}
  \end{align}
  \item Conjugacy: this {\bf posterior} has the same form as the {\bf prior}
\end{itemize}
\end{frame}




\begin{frame}{Generative Model}
	\only<1> {   \includegraphics[width=.3\linewidth]{topic_models/nyt_topics}  }
	\only<2> {   \includegraphics[width=.8\linewidth]{topic_models/nyt_documents}  }
	\only<3> {   \includegraphics[width=.8\linewidth]{topic_models/inference_0}  }
	\only<4> {   \includegraphics[width=.8\linewidth]{topic_models/inference_1}  }
	\only<5> {   \includegraphics[width=.8\linewidth]{topic_models/inference_2}  }
	\only<6> {   \includegraphics[width=.8\linewidth]{topic_models/inference_3}  }
\end{frame}

\frame
{
  \frametitle{Generative Model Approach}

\begin{center}
\only<1>{ \includegraphics[scale=0.4]{topic_models/lda1.pdf} }
\only<2>{ \includegraphics[scale=0.4]{topic_models/lda2.pdf} }
\only<3>{ \includegraphics[scale=0.4]{topic_models/lda3.pdf} }
\only<4->{ \includegraphics[scale=0.4]{topic_models/lda4.pdf} }
\end{center}

\begin{itemize}
\item<1-> For each topic $k \in \{1, \dots, K\}$, draw a multinomial distribution $\beta_k$ from a Dirichlet distribution with parameter $\lambda$
\item<2-> For each document $d \in \{1, \dots, M\}$, draw a multinomial distribution $\theta_d$ from a Dirichlet distribution with parameter $\alpha$
\item<3-> For each word position $n \in \{1, \dots, N\}$, select a hidden topic $z_n$ from the multinomial distribution parameterized by $\theta$.
\item<4-> Choose the observed word $w_n$ from the distribution $\beta_{z_n}$.
\end{itemize}

\only<5->{We use statistical inference to uncover the most likely unobserved variables given observed data.}
}

\begin{frame}
\frametitle{Topic Models: What's Important}
\begin{itemize}
\item Topic models \only<2>{(latent variables)}
\begin{itemize}
	\item Topics to word types---multinomial distribution
	\item Documents to topics---multinomial distribution
\end{itemize}
\item Focus in this talk: statistical methods
  \begin{itemize}
    \item Model: story of how your data came to be
    \item Latent variables: missing pieces of your story
    \item Statistical inference: filling in those missing pieces
  \end{itemize}
\item We use latent Dirichlet allocation (LDA), a fully Bayesian
  version of pLSI, probabilistic version of
  LSA
\end{itemize}

\end{frame}

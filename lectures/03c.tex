\documentclass[compress]{beamer}

\input{course_style}


\usepackage{tikz-dependency}
%\usepackage{tikz-qtree}
\usepackage{qtree}
\usepackage{pdfpages}

\newcommand{\gfx}[2]{
\begin{center}
	\includegraphics[width=#2\linewidth]{distsim/#1}
\end{center}
}
\title{Distributional Semantics}
\date{Slides Adapted from Yoav Goldberg and Omer Levy}

\begin{document}

\tikzstyle{every picture}+=[remember picture]



\begin{frame}
  \titlepage
\end{frame}


\begin{frame}{Beyond word2vec}

    \begin{itemize}
        \item word2vec is factorizing a word-context matrix.
        \item The content of this matrix affects the resulting similarities.
        \item word2vec allows you to specify a \textit{window size}.
        \item But what about other types of contexts?
    \end{itemize}

    \begin{itemize}
        \item Example: \textbf{dependency contexts} {\footnotesize (Levy and
            Dagan, ACL 2014)}
    \end{itemize}

\end{frame}

{
\setbeamercolor{background canvas}{bg=}
\includepdf[pages=-]{distsim/dep_embeddings.pdf}
}

\begin{frame}{Context matters}

    \begin{block}{Choose the correct contexts for your application}
        \begin{itemize}
            \item larger window sizes -- more topical
            \item dependency relations -- more functional
                \pause
            \item only noun-adjective relations
%                \scalebox{0.8}{
%                \begin{tabular}{cc}
%                    & \textbf{all dependencies} \\
%                            \hline
%                                   \textbf{sandwich}:
%                                   & satay, sandwiches, ravioli, doughnut, dessert, \\
%                                   & casserole, steak, shortbread, omelette, risotto \\
%                           \hline
%                           & \textbf{only noun-adj dependencies} \\
%                           \hline
%                                \textbf{sandwich}:
%                                & pizza, sandwiches, pan, pastry, doughnut \\
%                                & biscuit, doughnuts, burger, dessert, pies \\
%                            \end{tabular}}
%                \pause
            \item only verb-subject relations
                \pause
            \item context: time of the current message
            \item context: user who wrote the message
                \pause
            \item \ldots
            \item the sky is the limit
        \end{itemize}
    \end{block}

\end{frame}


\begin{frame}{Summary}
    \begin{block}{Distributional Semantics}
        \begin{itemize}
            \item Words in similar contexts have similar meanings.
            \item Represent a word by the contexts it appears in.
            \item But what is a context?
        \end{itemize}
    \end{block}
    \begin{block}{Neural Models (word2vec)}
        \begin{itemize}
            \item Represent each word as dense, low-dimensional vector.
            \item Same intuitions as in distributional vector-space models.
            \item Efficient to run, scales well, modest memory requirement.
            \item Dense vectors are convenient to work with.
            \item Still helpful to think of the context types.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}

%%%% examples
